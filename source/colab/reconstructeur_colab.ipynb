{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reconstructeur.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mmFvxk2BWATC",
        "colab_type": "code",
        "outputId": "be5876d4-9a6e-4a25-bfda-e00790c976c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_D_Nx0fEU00Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d4TS8ujSeLTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "376cd8a1-ce2f-45d1-d125-64c1032939e5"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"drive/Colab/data\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customShapeNet', 'ShapeNetRendering', 'latentVectors', 'output_clouds']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "pNHaNKwZcCE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2de00bc6-45a0-4186-f3f7-52a4c84332c8"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "accelerator\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cu80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "qAJsmrxrWVUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "06c733cf-61b2-4e53-84fc-d4ced5197a96"
      },
      "cell_type": "code",
      "source": [
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install torch\n",
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.1.post2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "c-vmDv7-YueV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQSOXpd71l-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def loadImage(path):\n",
        "    # Now that we have an img, we need to preprocess it.\n",
        "    img = Image.open(path)\n",
        "    \n",
        "    # We need to:\n",
        "    #       * resize the img, it is pretty big (~1200x1200px).\n",
        "    #       * normalize it, as noted in the PyTorch pretrained models doc,\n",
        "    #         with, mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "    #       * convert it to a PyTorch Tensor.\n",
        "    # We can do all this preprocessing using a transform pipeline.\n",
        "    img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
        "    transform_pipeline = transforms.Compose([transforms.Resize(img_size),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                  std=[0.229, 0.224, 0.225])])\n",
        "    \n",
        "    img = transform_pipeline(img)\n",
        "    \n",
        "    # PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
        "    # Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
        "    #img = img.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims.\n",
        "    img = img[:3, :, :].unsqueeze(0)  #enlève le channel alpha\n",
        "    \n",
        "    # Now that we have preprocessed our img, we need to convert it into a\n",
        "    # Variable; PyTorch models expect inputs to be Variables. A PyTorch Variable is a\n",
        "    # wrapper around a PyTorch Tensor.\n",
        "    return torch.tensor(img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RNuQttsozwM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "ply_dtypes = dict([\n",
        "    (b'int8', 'i1'),\n",
        "    (b'char', 'i1'),\n",
        "    (b'uint8', 'u1'),\n",
        "    (b'uchar', 'b1'),\n",
        "    (b'uchar', 'u1'),\n",
        "    (b'int16', 'i2'),\n",
        "    (b'short', 'i2'),\n",
        "    (b'uint16', 'u2'),\n",
        "    (b'ushort', 'u2'),\n",
        "    (b'int32', 'i4'),\n",
        "    (b'int', 'i4'),\n",
        "    (b'uint32', 'u4'),\n",
        "    (b'uint', 'u4'),\n",
        "    (b'float32', 'f4'),\n",
        "    (b'float', 'f4'),\n",
        "    (b'float64', 'f8'),\n",
        "    (b'double', 'f8')\n",
        "])\n",
        "\n",
        "valid_formats = {'ascii': '', 'binary_big_endian': '>',\n",
        "                 'binary_little_endian': '<'}\n",
        "\n",
        "\n",
        "def read_ply(filename):\n",
        "    \"\"\" Read a .ply (binary or ascii) file and store the elements in pandas DataFrame\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename: str\n",
        "        Path tho the filename\n",
        "    Returns\n",
        "    -------\n",
        "    data: dict\n",
        "        Elements as pandas DataFrames; comments and ob_info as list of string\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, 'rb') as ply:\n",
        "\n",
        "        if b'ply' not in ply.readline():\n",
        "            raise ValueError('The file does not start whith the word ply')\n",
        "        # get binary_little/big or ascii\n",
        "        fmt = ply.readline().split()[1].decode()\n",
        "        # get extension for building the numpy dtypes\n",
        "        ext = valid_formats[fmt]\n",
        "\n",
        "        line = []\n",
        "        dtypes = defaultdict(list)\n",
        "        count = 2\n",
        "        points_size = None\n",
        "        mesh_size = None\n",
        "        while b'end_header' not in line and line != b'':\n",
        "            line = ply.readline()\n",
        "\n",
        "            if b'element' in line:\n",
        "                line = line.split()\n",
        "                name = line[1].decode()\n",
        "                size = int(line[2])\n",
        "                if name == \"vertex\":\n",
        "                    points_size = size\n",
        "                elif name == \"face\":\n",
        "                    mesh_size = size\n",
        "\n",
        "            elif b'property' in line:\n",
        "                line = line.split()\n",
        "                # element mesh\n",
        "                if b'list' in line:\n",
        "                    mesh_names = ['n_points', 'v1', 'v2', 'v3']\n",
        "\n",
        "                    if fmt == \"ascii\":\n",
        "                        # the first number has different dtype than the list\n",
        "                        dtypes[name].append(\n",
        "                            (mesh_names[0], ply_dtypes[line[2]]))\n",
        "                        # rest of the numbers have the same dtype\n",
        "                        dt = ply_dtypes[line[3]]\n",
        "                    else:\n",
        "                        # the first number has different dtype than the list\n",
        "                        dtypes[name].append(\n",
        "                            (mesh_names[0], ext + ply_dtypes[line[2]]))\n",
        "                        # rest of the numbers have the same dtype\n",
        "                        dt = ext + ply_dtypes[line[3]]\n",
        "\n",
        "                    for j in range(1, 4):\n",
        "                        dtypes[name].append((mesh_names[j], dt))\n",
        "                else:\n",
        "                    if fmt == \"ascii\":\n",
        "                        dtypes[name].append(\n",
        "                            (line[2].decode(), ply_dtypes[line[1]]))\n",
        "                    else:\n",
        "                        dtypes[name].append(\n",
        "                            (line[2].decode(), ext + ply_dtypes[line[1]]))\n",
        "            count += 1\n",
        "\n",
        "        # for bin\n",
        "        end_header = ply.tell()\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    if fmt == 'ascii':\n",
        "        top = count\n",
        "        bottom = 0 if mesh_size is None else mesh_size\n",
        "\n",
        "        names = [x[0] for x in dtypes[\"vertex\"]]\n",
        "\n",
        "        data[\"points\"] = pd.read_csv(filename, sep=\" \", header=None, engine=\"python\",\n",
        "                                     skiprows=top, skipfooter=bottom, usecols=names, names=names)\n",
        "\n",
        "        for n, col in enumerate(data[\"points\"].columns):\n",
        "            data[\"points\"][col] = data[\"points\"][col].astype(\n",
        "                dtypes[\"vertex\"][n][1])\n",
        "\n",
        "        if mesh_size is not None:\n",
        "            top = count + points_size\n",
        "\n",
        "            names = [x[0] for x in dtypes[\"face\"]][1:]\n",
        "            usecols = [1, 2, 3]\n",
        "\n",
        "            data[\"mesh\"] = pd.read_csv(\n",
        "                filename, sep=\" \", header=None, engine=\"python\", skiprows=top, usecols=usecols, names=names)\n",
        "\n",
        "            for n, col in enumerate(data[\"mesh\"].columns):\n",
        "                data[\"mesh\"][col] = data[\"mesh\"][col].astype(\n",
        "                    dtypes[\"face\"][n + 1][1])\n",
        "\n",
        "    else:\n",
        "        with open(filename, 'rb') as ply:\n",
        "            ply.seek(end_header)\n",
        "            data[\"points\"] = pd.DataFrame(np.fromfile(\n",
        "                ply, dtype=dtypes[\"vertex\"], count=points_size))\n",
        "            if mesh_size is not None:\n",
        "                data[\"mesh\"] = pd.DataFrame(np.fromfile(\n",
        "                    ply, dtype=dtypes[\"face\"], count=mesh_size))\n",
        "                data[\"mesh\"].drop('n_points', axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def write_ply(filename, points=None, faces = None, mesh=None, as_text=False, normal=False, text=False, color = False):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename: str\n",
        "        The created file will be named with this\n",
        "    points: ndarray\n",
        "    mesh: ndarray\n",
        "    as_text: boolean\n",
        "        Set the write mode of the file. Default: binary\n",
        "    Returns\n",
        "    -------\n",
        "    boolean\n",
        "        True if no problems\n",
        "    \"\"\"\n",
        "    if not filename.endswith('ply'):\n",
        "        filename += '.ply'\n",
        "\n",
        "    # open in text mode to write the header\n",
        "    with open(filename, 'w') as ply:\n",
        "        header = ['ply']\n",
        "\n",
        "        if as_text:\n",
        "            header.append('format ascii 1.0')\n",
        "        else:\n",
        "            header.append('format binary_' + sys.byteorder + '_endian 1.0')\n",
        "\n",
        "        if points is not None:\n",
        "            header.extend(describe_element('vertex', points, normal, text, color))\n",
        "        if mesh is not None:\n",
        "            mesh = mesh.copy()\n",
        "            mesh.insert(loc=0, column=\"n_points\", value=3)\n",
        "            mesh[\"n_points\"] = mesh[\"n_points\"].astype(\"u1\")\n",
        "            header.extend(describe_element('face', mesh, normal, text, color))\n",
        "        if faces is not None:\n",
        "            header.extend(describe_element('face', faces, text))\n",
        "        header.append('end_header')\n",
        "\n",
        "        for line in header:\n",
        "            ply.write(\"%s\\n\" % line)\n",
        "\n",
        "    if as_text:\n",
        "        if points is not None:\n",
        "            # print(\"write points\")\n",
        "            points.to_csv(filename, sep=\" \", index=False, header=False, mode='a',\n",
        "                          encoding='ascii')\n",
        "            # print(\"end points\")\n",
        "        \n",
        "        if faces is not None:\n",
        "            faces.to_csv(filename, sep=\" \", index=False, header=False, mode='a',\n",
        "                          encoding='ascii')\n",
        "        \n",
        "        if mesh is not None:\n",
        "            mesh.to_csv(filename, sep=\" \", index=False, header=False, mode='a',\n",
        "                        encoding='ascii')\n",
        "\n",
        "    else:\n",
        "        # open in binary/append to use tofile\n",
        "        with open(filename, 'ab') as ply:\n",
        "            if points is not None:\n",
        "                points.to_records(index=False).tofile(ply)\n",
        "            if faces is not None:\n",
        "                faces.to_records(index=False).tofile(ply)\n",
        "            if mesh is not None:\n",
        "                mesh.to_records(index=False).tofile(ply)\n",
        "        # Read in the file\n",
        "    # with open(filename, 'r') as file :\n",
        "    #   filedata = file.read()\n",
        "\n",
        "    # # Replace the target string\n",
        "    # filedata = filedata.replace('.', ',')\n",
        "    # filedata = filedata.replace('ascii 1,0', 'ascii 1.0')\n",
        "\n",
        "    # # Write the file out again\n",
        "    # with open(filename, 'w') as file:\n",
        "    #   file.write(filedata)\n",
        "    return True\n",
        "\n",
        "\n",
        "def describe_element(name, df, normal=False, text = False, color=False):\n",
        "    \"\"\" Takes the columns of the dataframe and builds a ply-like description\n",
        "    Parameters\n",
        "    ----------\n",
        "    name: str\n",
        "    df: pandas DataFrame\n",
        "    Returns\n",
        "    -------\n",
        "    element: list[str]\n",
        "    \"\"\"\n",
        "    property_formats = {'f': 'float', 'u': 'uchar', 'i': 'int'}\n",
        "    element = ['element ' + name + ' ' + str(len(df))]\n",
        "\n",
        "    if name == 'face':\n",
        "        element.append(\"property list uchar int vertex_indices\")\n",
        "\n",
        "    else:\n",
        "        element.append('property float x')\n",
        "        element.append('property float y')\n",
        "        element.append('property float z')\n",
        "        \n",
        "        if text:\n",
        "           element.append('property float u')\n",
        "           element.append('property float v')\n",
        "        \n",
        "        if normal:\n",
        "            element.append('property float nx')\n",
        "            element.append('property float ny')\n",
        "            element.append('property float nz')\n",
        "        if color:\n",
        "            element.append('property uchar red')\n",
        "            element.append('property uchar green')\n",
        "            element.append('property uchar blue')\n",
        "\n",
        "    return element\n",
        "\n",
        "\n",
        "\n",
        "def _gen_latent(root, id_category, nPerCat, nPerObj):\n",
        "    \"\"\"renvoie un vecteur d'images chargées dans l'ordre\n",
        "    taille: id_category * nPerCat * nPerObj\"\"\"\n",
        "    res = []\n",
        "    for cat in id_category:\n",
        "        path = root + str(cat) + \"/\"\n",
        "        for key in os.listdir(path)[:nPerCat]:\n",
        "            sub_path = path + key + \"/rendering/\"\n",
        "            \n",
        "            with open(sub_path + \"renderings.txt\") as f:\n",
        "                cpt = 0\n",
        "                for line in f:\n",
        "                    res.append(loadImage(sub_path + line[:-1]))\n",
        "                    cpt += 1\n",
        "                    if cpt == nPerObj:\n",
        "                        break\n",
        "    \n",
        "    return res  #torch.utils.data.DataLoader(res, nPerCat)\n",
        "\n",
        "\n",
        "def _gen_clouds(root, id_category, nPerCat, ratio_sous_echantillonage):\n",
        "    root += \"/\"\n",
        "    res = []\n",
        "    for cat in id_category:\n",
        "        path = root + str(cat) + \"/ply/\"\n",
        "        cpt = 0\n",
        "        for key in os.listdir(path):\n",
        "            #ignore les fichiers .txt\n",
        "            if key[-4:] != \".txt\":\n",
        "                sub_path = path + key\n",
        "                cloud = read_ply(sub_path)['points']\n",
        "                \n",
        "                sub_sampled = []\n",
        "                for i, x in enumerate(cloud.values[:, :3]):\n",
        "                    if len(sub_sampled) < ratio_sous_echantillonage * (i+1):\n",
        "                        sub_sampled.append(torch.tensor(x))\n",
        "                        \n",
        "                res.append(sub_sampled)\n",
        "                \n",
        "                cpt += 1\n",
        "                if cpt == nPerCat:\n",
        "                    break\n",
        "    \n",
        "    return np.array(res, dtype=torch.Tensor)\n",
        "\n",
        "\n",
        "def _save_latent(latent, root, id_category, nPerCat, nPerObj):\n",
        "    cpt=0\n",
        "    for cat in id_category:\n",
        "        path = root + \"/\" + str(cat)\n",
        "        if not os.path.isdir(path):\n",
        "            os.mkdir(path)\n",
        "        for ind_obj in range(nPerCat):\n",
        "            path2 = path + \"/\" + str(ind_obj)\n",
        "            if not os.path.isdir(path2):\n",
        "                os.mkdir(path2)\n",
        "            for ind_view in range(nPerObj):\n",
        "                path3 = path2 + \"/\" + str(ind_view)\n",
        "                if not os.path.isfile(path3):\n",
        "                    np.save(path3, latent[cpt])\n",
        "                cpt+=1\n",
        "\n",
        "\n",
        "def _load_latent(root, id_category, nPerCat, nPerObj):\n",
        "    latent = []\n",
        "    for cat in id_category:\n",
        "        path = root + \"/\" + str(cat)\n",
        "        for ind_obj in range(nPerCat):\n",
        "            path2 = path + \"/\" + str(ind_obj)\n",
        "            for ind_view in range(nPerObj):\n",
        "                path3 = path2 + \"/\" + str(ind_view) + \".npy\"\n",
        "                latent.append(torch.tensor(np.load(path3)))\n",
        "    return latent\n",
        "\n",
        "\n",
        "def _get_categories(path, chosenSubSet):\n",
        "    path += \"/synsetoffset2category.txt\"\n",
        "    \n",
        "    id_category = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            id_category.append(line.split()[1])\n",
        "    \n",
        "    #récupère les catégories des indices demandés\n",
        "    if chosenSubSet:\n",
        "        id_category = [id_category[i] for i in chosenSubSet]\n",
        "    return id_category\n",
        "\n",
        "\n",
        "def get_latent(chosenSubSet, nPerCat, nPerObj):\n",
        "    id_category = _get_categories(\"drive/Colab/data/ShapeNetRendering/\", chosenSubSet)\n",
        "    root = \"../AtlasNet/data/ShapeNetRendering\"\n",
        "    local_path = \"drive/Colab/data/latentVectors\"\n",
        "    \n",
        "    #chargement des vecteurs latents\n",
        "    latentVectors = _load_latent(local_path, id_category, nPerCat, nPerObj)\n",
        "    \n",
        "    return latentVectors\n",
        "\n",
        "\n",
        "def get_clouds(chosenSubSet, nPerCat, ratio):\n",
        "    assert 0 < ratio <= 1\n",
        "    path = \"drive/Colab/data/customShapeNet\"\n",
        "    id_category = _get_categories(path, chosenSubSet)\n",
        "    return _gen_clouds(path, id_category, nPerCat, ratio)\n",
        "\n",
        "\n",
        "def write_clouds(path, clouds):\n",
        "    if os.path.isdir(path):\n",
        "        print(path, \"écrasé\")\n",
        "    else :\n",
        "        print(path, \"écrit\")\n",
        "        os.mkdir(path)\n",
        "    path += '/'\n",
        "    for i, c in enumerate(clouds):\n",
        "        ply.write_ply(path + str(i), pd.DataFrame(c), as_text=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xMMUHj5kTg3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f10b8dc5-d8cb-4d7f-bb75-a569ea9b1c61"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qTBkvUMEyOuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Segmentation:\n",
        "    \"\"\"défini la manière de partitionner l'espace 3D en cases\"\"\"\n",
        "    \n",
        "    def __init__(self, n_step):\n",
        "        assert n_step % 2 == 1, \"n_step doit être impair pour que le centre soit une cellule\"\n",
        "        self.dim = 3\n",
        "        self.coo_min = -1\n",
        "        self.coo_max = 1\n",
        "        self.n_step = n_step\n",
        "        self.n_step_2 = n_step / 2\n",
        "        \n",
        "        self.neighbours, self.distances_2 = self._gen_neighbours()\n",
        "        # for d in self.distances_2:\n",
        "        #     print(d,\":\",self.neighbours[d])\n",
        "        \n",
        "        # sert à initialiser la matrice\n",
        "        self.filler = np.frompyfunc(lambda x: list(), 1, 1)\n",
        "    \n",
        "    def _gen_neighbours(self):\n",
        "        # génère tous les déplacements possibles de -n à n dans chaque dim\n",
        "        r = range(self.n_step * 2)\n",
        "        list_neighbours = np.array([[[(i, j, k) for i in r] for j in r] for k in r])\n",
        "        \n",
        "        # shift de n pour centrer\n",
        "        list_neighbours = [voisin - self.n_step for voisin in list_neighbours.reshape(-1, 3)]\n",
        "        \n",
        "        # facteur pour passer de distances entre cases (coordonnées entières) à des distances entre points\n",
        "        factor_2 = ((self.coo_max - self.coo_min) / self.n_step) ** 2\n",
        "        \n",
        "        # trie par distance au centre\n",
        "        # il y a bcp d'équivalents\n",
        "        # on regroupe les équivalents dans une même liste\n",
        "        dict_neighbours = {}\n",
        "        for voisin in list_neighbours:\n",
        "            # la distance minimale entre des points des deux cases est égale à\n",
        "            # la norme du vecteur entre les cases auquel on ENLEVE 1 DANS TOUTES LES DIMENSIONS NON NULLES\n",
        "            # on laisse tout au carré\n",
        "            d2 = np.sum(np.power(np.maximum(np.abs(voisin) - 1, 0), 2)) * factor_2\n",
        "            \n",
        "            # une dimension égale à 1 devient équivalente à une dimension nulle\n",
        "            # on ajoute un terme négligeable pour mettre en premiers les cases les plus proches du centre\n",
        "            d2 += (np.abs(voisin) == 1).sum() * 1e-6\n",
        "            \n",
        "            if d2 not in dict_neighbours:  # pas de pb d'arrrondi car on travaille sur des entiers, puis effectue les mêmes opérations\n",
        "                dict_neighbours[d2] = []\n",
        "            \n",
        "            dict_neighbours[d2].append(voisin)\n",
        "        \n",
        "        dict_neighbours = {k: np.array(v) for (k, v) in dict_neighbours.items()}\n",
        "        \n",
        "        # calcule la liste triée des distances\n",
        "        list_dist_2 = np.array(sorted(list(dict_neighbours.keys())))\n",
        "        \n",
        "        return dict_neighbours, list_dist_2\n",
        "    \n",
        "    def get_neighbours(self, cell, dist):\n",
        "        \"\"\"retourne la liste des cases à distance dist de cell\"\"\"\n",
        "        # ajoute notre position pour calculer les coordonnées des voisins\n",
        "        res = cell + self.neighbours[dist]\n",
        "        \n",
        "        # filtre les voisins qui sortent du tableau\n",
        "        return [v for v in res if np.all((0 <= v) & (v < self.n_step))]\n",
        "    \n",
        "    def gen_matrix(self):\n",
        "        \"\"\"retourne un tableau 3D servant à stocker les points\"\"\"\n",
        "        a = np.empty((self.n_step, self.n_step, self.n_step), dtype=list)\n",
        "        return self.filler(a, a)\n",
        "    \n",
        "    def get_mat_coo(self, point):\n",
        "        \"\"\"retourne les coordonnées de la case du tableau contenant ce point\"\"\"\n",
        "        res = [int((coo + 1) * self.n_step_2) for coo in point]\n",
        "        #<=> int((coo - self.coo_min) / (self.coo_max - self.coo_min) * self.n_step)\n",
        "        \n",
        "        # si une coordonnée est maximale (càd 1), le point sort du tableau, on le met dans la case d'avant\n",
        "        return [min(x, self.n_step - 1) for x in res]\n",
        "    \n",
        "    def get_float_coo(self, point):\n",
        "        \"\"\"retourne les coordonnées de notre point dans la base des cases du tableau\"\"\"\n",
        "        return np.minimum((point + 1) * self.n_step_2, self.n_step - 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCshIqj6ydkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Nuage:\n",
        "    \"\"\"stocke l'ensemble des points sous la forme\n",
        "    d'une liste de points et\n",
        "    d'un tableau à 3 dimensions\"\"\"\n",
        "    \n",
        "    def __init__(self, points, segmentation):\n",
        "        self.segmentation = segmentation\n",
        "        self.liste_points = points\n",
        "        \n",
        "        # crée le tableau\n",
        "        self.mat = segmentation.gen_matrix()\n",
        "        \n",
        "        # remplit le tableau\n",
        "        for p in self.liste_points:\n",
        "            x, y, z = self.segmentation.get_mat_coo(p)\n",
        "            self.mat[x, y, z].append(p)\n",
        "        \n",
        "    def recreate(self, points):\n",
        "        \"\"\"vide et re-rempli l'ancien tableau plutôt que de le supprimer et d'en allouer un nouveau\"\"\"\n",
        "        self.liste_points = points\n",
        "        \n",
        "        # vide\n",
        "        self.segmentation.filler(self.mat, self.mat)\n",
        "        \n",
        "        # remplit\n",
        "        for p in self.liste_points:\n",
        "            x, y, z = self.segmentation.get_mat_coo(p)\n",
        "            self.mat[x, y, z].append(p)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "\n",
        "    # pour afficher des statistiques: liste du nombre de points parcourus pour chaque appel à get_closest\n",
        "    points_parcourus = []\n",
        "    \n",
        "    def get_closest(self, point):\n",
        "        # calcule la case dans laquelle tomberait le point\n",
        "        case_coo = self.segmentation.get_mat_coo(point)\n",
        "        \n",
        "        # le point le plus proche ne se trouve pas forcément dans la case la plus proche\n",
        "        # un point plus proche peut encore être trouvé dans une case plus lointaine tant que\n",
        "        # la distance à l'angle le plus proche de cette case est inférieur au min actuel\n",
        "        point_coo = self.segmentation.get_float_coo(point.to(\"cpu\").detach().numpy())\n",
        "        \n",
        "        cpt_point_parcourus = 0\n",
        "        closest_point = torch.tensor(float(\"inf\")).to(device)\n",
        "        for d_2 in self.segmentation.distances_2:\n",
        "            # stoppe la recherche si la distance minimale entre les angles des deux cases n'est pas meilleure\n",
        "            if d_2 >= closest_point:\n",
        "                break\n",
        "            \n",
        "            # considère toutes les cases non vides à distance d de notre case\n",
        "            neighbours = [v for v in self.segmentation.get_neighbours(case_coo, d_2) if self.mat[v[0], v[1], v[2]]]\n",
        "            \n",
        "            # calcule la distance entre notre point et le sommet de la case le plus proche\n",
        "            precise_dist_2 = [(np.sum(np.power(point_coo - v, 2)), v) for v in neighbours]\n",
        "            \n",
        "            # enlève les cases dont la distance min est trop grande\n",
        "            precise_dist_2 = [(d_2, v) for (d_2, v) in precise_dist_2 if d_2 < closest_point]\n",
        "            \n",
        "            # parcourt les cases par ordre croissant (pour réduire l'espérance de temps pour trouver le point le plus proche)\n",
        "            for real_d_2, v in sorted(precise_dist_2, key=lambda x: x[0]):\n",
        "                if real_d_2 > closest_point:\n",
        "                    break\n",
        "                    \n",
        "                cpt_point_parcourus += len(self.mat[v[0], v[1], v[2]])\n",
        "                \n",
        "                # regarde si le point le plus proche parmi tous les points de la case est mieux\n",
        "                candidat = torch.min(torch.cat([torch.sum(torch.pow(point - p, 2), dim=(0,)).unsqueeze(0) for p in self.mat[v[0], v[1], v[2]]]))\n",
        "                if candidat < closest_point:\n",
        "                    closest_point = candidat\n",
        "            else:\n",
        "                continue  # only executed if the inner loop did NOT break\n",
        "            break  # only executed if the inner loop DID break\n",
        "        \n",
        "        Nuage.points_parcourus.append(cpt_point_parcourus)\n",
        "        # return torch.sqrt(closest_point)\n",
        "        return closest_point\n",
        "    \n",
        "    def chamfer(self, other):\n",
        "        loss0 = torch.sum(torch.cat([self.get_closest(p).unsqueeze(0) for p in other.liste_points]))\n",
        "        loss1 = torch.sum(torch.cat([other.get_closest(p).unsqueeze(0) for p in self.liste_points]))\n",
        "        return loss0, loss1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VXf9rX5ykgb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Reconstructeur(nn.Module):\n",
        "    def __init__(self, n_mlp, latent_size, n_grid_step, segmentation):\n",
        "        super().__init__()\n",
        "        print(\"nombre de MLP:\", n_mlp)\n",
        "        print(\"résolution de la grille:\", n_grid_step, \"^2 =\", n_grid_step ** 2)\n",
        "        print(\"taille des nuages générés:\", n_mlp, \"*\", n_grid_step ** 2, \"=\", n_mlp * n_grid_step ** 2)\n",
        "        print(\"résolution segmentation:\", segmentation.n_step, \"^3 =\", segmentation.n_step ** 3)\n",
        "        self._nuage_tmp = Nuage([], segmentation)\n",
        "        self.input_dim = 2\n",
        "        self.output_dim = 3\n",
        "        self.n_grid_step = n_grid_step\n",
        "        self.verbose = True\n",
        "        \n",
        "        self.decodeurs = nn.ModuleList([nn.Sequential(\n",
        "            #on rajoute les coordonnées d'un point dans l'espace de départ\n",
        "            nn.Linear(latent_size + self.input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            #on retourne un point dans l'espace de sortie\n",
        "            nn.Linear(128, self.output_dim),\n",
        "            #tanh ramène dans [-1, 1], comme les nuages de notre auto-encodeur et de groueix\n",
        "            nn.Tanh()\n",
        "        ) for _ in range(n_mlp)])\n",
        "        \n",
        "        self.grid = self._gen_grid()\n",
        "        self.loss = Nuage.chamfer\n",
        "    \n",
        "    def _gen_grid(self):\n",
        "        \"\"\"retourne une liste de n points 2D au bon format :\n",
        "        array(tensor(coo_x, coo_y))\n",
        "        les points forment un quadrillage du carré unitaire (il n'y a pas d'aléa)\n",
        "        les tensors ont requires_grad=False car cela reviendrait à déformer la grille\"\"\"\n",
        "        points_dim = [np.linspace(-1, 1, self.n_grid_step) for _ in range(self.input_dim)]\n",
        "        points_dim = np.meshgrid(*points_dim)\n",
        "        \n",
        "        def f(*point_dim):\n",
        "            v = torch.tensor(list(point_dim)).float()\n",
        "            v.requires_grad = False\n",
        "            return v.to(device)\n",
        "        \n",
        "        res = np.vectorize(f, otypes=[object])(*points_dim).reshape(-1)\n",
        "        return res\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # concatenation de x et p pour tous les points de la grille\n",
        "        grid_x = [torch.cat((x, p)) for p in self.grid]\n",
        "        \n",
        "        # f(x_p) pour tout x_p pour tout mlp\n",
        "        return self._nuage_tmp.recreate(np.array([[f(x_p) for x_p in grid_x] for f in self.decodeurs]).reshape(-1))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JScJs-99ymq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_reconstructeur(reconstructeur, x_train, y_train, x_test, y_test, epochs, lr=1e-5, grid_scale=1.0):\n",
        "    \"\"\"grid_scale:\n",
        "        les points 3D générés sont calculés à partir d'un échantillonage d'un carré 1*1,\n",
        "        prendre un carré 100*100 revient à augmenter les coefficients du\n",
        "        premier Linear sans pénaliser le modèle (Adam y est en fait insensible par défaut)\"\"\"\n",
        "    reconstructeur.grid *= grid_scale\n",
        "    optimizer = torch.optim.Adam(reconstructeur.parameters(), lr=lr)\n",
        "    loss_train = None\n",
        "    \n",
        "    time_loss = np.zeros(epochs)\n",
        "    time_tot = np.zeros(epochs)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_train = 0\n",
        "        t_tot = time.time()\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            assert not x.requires_grad\n",
        "            \n",
        "            y_pred = reconstructeur.forward(x)\n",
        "            \n",
        "            t_loss = time.time()\n",
        "            loss = reconstructeur.loss(y_pred, y)\n",
        "            time_loss[epoch] += time.time() - t_loss\n",
        "            \n",
        "            # print(loss)\n",
        "            loss = loss[0] + loss[1]\n",
        "            loss_train += loss.item() / len(x_train)\n",
        "            \n",
        "            # Zero gradients, perform a backward pass, and update the weights.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        time_tot[epoch] += time.time() - t_tot\n",
        "        \n",
        "        if reconstructeur.verbose and (epochs < 10 or epoch % (epochs // 10) == 0):\n",
        "            reconstructeur.eval()\n",
        "            \n",
        "            s_test = sum(sum(reconstructeur.loss(reconstructeur.forward(x), y.data)).item() for (x, y) in zip(x_test, y_test))\n",
        "            \n",
        "            print(\"time\", epoch,\n",
        "                  \"loss train %.4e\" % loss_train,\n",
        "                  \"loss test %.2e\" % s_test)\n",
        "            reconstructeur.train()\n",
        "        \n",
        "        if loss_train / len(x_train) < 1e-8:\n",
        "            #convergence finie, ca ne sert à rien de continuer\n",
        "            break\n",
        "    \n",
        "    #apprentissage fini, passage en mode évaluation\n",
        "    reconstructeur.eval()\n",
        "    return loss_train, time_loss, time_tot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qqyAlvGx1veI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Reconstructeur2(nn.Module):\n",
        "    \"\"\"chamfer quadratique, à supprimer\"\"\"\n",
        "    def __init__(self, n_mlp, latent_size, n_grid_step):\n",
        "        super().__init__()\n",
        "        print(\"nombre de MLP:\", n_mlp)\n",
        "        print(\"résolution de la grille:\", n_grid_step, \"*\", n_grid_step, \"=\", n_grid_step ** 2)\n",
        "        print(\"taille des nuages générés:\", n_mlp, \"*\", n_grid_step ** 2, \"=\", n_mlp * n_grid_step ** 2, )\n",
        "        self.input_dim = 2\n",
        "        self.output_dim = 3\n",
        "        self.n_grid_step = n_grid_step\n",
        "        self.verbose = True\n",
        "        \n",
        "        self.decodeurs = nn.ModuleList([nn.Sequential(\n",
        "            #on rajoute les coordonnées d'un point dans l'espace de départ\n",
        "            nn.Linear(latent_size + self.input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            #on retourne un point dans l'espace de sortie\n",
        "            nn.Linear(128, self.output_dim),\n",
        "            #tanh ramène dans [-1, 1], comme les nuages de notre auto-encodeur et de groueix\n",
        "            nn.Tanh()\n",
        "        ) for _ in range(n_mlp)])\n",
        "        \n",
        "        self.decodeurs = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(latent_size + self.input_dim, self.output_dim),\n",
        "            nn.Tanh()\n",
        "        ) for _ in range(n_mlp)])\n",
        "        \n",
        "        self.grid = self._gen_grid()\n",
        "        self.loss = self.chamfer\n",
        "    \n",
        "    def _gen_grid(self):\n",
        "        points_dim = [np.linspace(0, 1, self.n_grid_step) for _ in range(self.input_dim)]\n",
        "        points_dim = np.meshgrid(*points_dim)\n",
        "        \n",
        "        def f(*point_dim):\n",
        "            v = torch.tensor(list(point_dim)).float()\n",
        "            v.requires_grad = False\n",
        "            return v\n",
        "        \n",
        "        return np.vectorize(f, otypes=[object])(*points_dim).reshape(-1)\n",
        "    \n",
        "    @staticmethod\n",
        "    def chamfer(Y, S):\n",
        "        \"\"\"return chamfer loss between\n",
        "        the generated set Y = {f(x, p) for each f, p}\n",
        "        and the real pointcloud S corresponding to the latent vector x\n",
        "\n",
        "        sum_F(sum_A) et min_A(min_F) correspondent à une simple somme ou min sur l'ensemble des données\n",
        "        donc on représente l'ensemble des points générés Y comme une liste et non comme une matrice\n",
        "        \"\"\"\n",
        "    \n",
        "        # Pas la même taille :\n",
        "        # normes = torch.pow(torch.norm(Y - S), 2)\n",
        "    \n",
        "        # normes = np.array([[torch.norm(y - s) for s in S] for y in Y])\n",
        "        normes = np.array([[torch.sum(torch.pow(y - s, 2), dim=(0,)) for s in S] for y in Y])\n",
        "        loss1 = np.sum(np.min(normes, axis=0))\n",
        "        loss2 = np.sum(np.min(normes, axis=1))\n",
        "        return loss1, loss2\n",
        "    \n",
        "        # On sert uniquement des min selon les deux axes, on peut ne pas stocker la matrice pour éviter les problèmes de RAM\n",
        "        # listes des minimums sur chaque ligne et colonne\n",
        "        min_axe0 = [torch.tensor(float(\"+inf\"))] * len(S)\n",
        "        min_axe1 = [torch.tensor(float(\"+inf\"))] * len(Y)\n",
        "    \n",
        "        # pour chaque case de la matrice, met à jour les deux minimums correspondants\n",
        "        for i, s in enumerate(S):\n",
        "            for j, y in enumerate(Y):\n",
        "                val = torch.norm(y - s)\n",
        "                min_axe0[i] = torch.min(val, min_axe0[i])\n",
        "                min_axe1[j] = torch.min(val, min_axe1[j])\n",
        "    \n",
        "        # les Q doivent bien être atteint par un MLP\n",
        "        loss1 = sum(min_axe0)\n",
        "    \n",
        "        # les MLP doivent bien atteindre un Q\n",
        "        loss2 = sum(min_axe1)\n",
        "    \n",
        "        return loss1, loss2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # concatenation de x et p pour tous les points de la grille\n",
        "        grid_x = [torch.cat((x, p)) for p in self.grid]\n",
        "        \n",
        "        # f(x_p) pour tout x_p pour tout mlp\n",
        "        return np.array([[f(x_p) for x_p in grid_x] for f in self.decodeurs]).reshape(-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "baYlBxKty1S5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "1637c52b-2317-445c-cd98-f2f17b6ee075"
      },
      "cell_type": "code",
      "source": [
        "chosen_subset = [0]\n",
        "n_per_cat = 1\n",
        "clouds2 = get_clouds(chosen_subset, n_per_cat, ratio=.01)\n",
        "clouds2 = [np.array([x.to(device) for x in c]) for c in clouds2]\n",
        "print(\"taille des nuages ground truth:\", len(clouds2[0]))\n",
        "latent = get_latent(chosen_subset, n_per_cat,\n",
        "                         nPerObj=1)  # /!\\ attention: il faut que les fichiers sur le disque correspondent\n",
        "latent = [x.to(device) for x in latent]\n",
        "\n",
        "segmentation = Segmentation(5)\n",
        "\n",
        "\n",
        "clouds = [Nuage(x, segmentation) for x in clouds2]\n",
        "\n",
        "# plot_tailles(clouds)\n",
        "#write_clouds(\"./data/output_clouds/ground_truth\", [[p.detach().numpy() for p in clouds[0]]])\n",
        "\n",
        "ratio_train = 1\n",
        "n_train = int(ratio_train * len(latent))\n",
        "\n",
        "indexes = list(range(len(latent)))\n",
        "#random.shuffle(indexes)\n",
        "train_x = [latent[i] for i in indexes[:n_train]]\n",
        "test_x = [latent[i] for i in indexes[n_train:]]\n",
        "train_y = [clouds[i] for i in indexes[:n_train]]\n",
        "test_y = [clouds[i] for i in indexes[n_train:]]\n",
        "train_y2 = [clouds2[i] for i in indexes[:n_train]]\n",
        "test_y2 = [clouds2[i] for i in indexes[n_train:]]\n",
        "\n",
        "n_mlp = 10\n",
        "latent_size = 25088  # défini par l'encodeur utilisé\n",
        "grid_points = 8\n",
        "epochs = 5\n",
        "grid_size = 1e0\n",
        "lr = 1e-4\n",
        "\n",
        "reconstructeur = Reconstructeur(n_mlp, latent_size, grid_points, segmentation)\n",
        "reconstructeur.to(device)\n",
        "\n",
        "t = time.time()\n",
        "for _ in range(1):\n",
        "    loss, t_loss, t_tot = fit_reconstructeur(reconstructeur, train_x, train_y, test_x, test_y, epochs, lr=lr, grid_scale=grid_size)\n",
        "    print(\"\\ntemps: \")\n",
        "    print(\"loss\", sum(t_loss), t_loss)\n",
        "    print(\"tot\", sum(t_tot), t_tot)\n",
        "    print(\"ratio\", sum(t_loss)/sum(t_tot), t_loss/t_tot)\n",
        "print(\"O(n)\", time.time() - t)\n",
        "\n",
        "print(\"répartition point parcourus\")\n",
        "print(np.histogram(Nuage.points_parcourus))\n",
        "print(np.mean(Nuage.points_parcourus))\n",
        "# print(segmentation.distances_2)\n",
        "\n",
        "output = reconstructeur.forward(latent[0])\n",
        "output = [p.cpu().detach().numpy() for p in output.liste_points]\n",
        "write_clouds(\"drive/Colab/data/output_clouds\", [output])\n",
        "\n",
        "exit()\n",
        "print(\"reconstructeur O(n2) : \")\n",
        "reconstructeur2 = Reconstructeur2(n_mlp, latent_size, grid_points)\n",
        "t = time.time()\n",
        "for _ in range(1):\n",
        "    fit_reconstructeur(reconstructeur2, train_x, train_y2, test_x, test_y2, epochs, lr=lr, grid_scale=grid_size)\n",
        "print(\"O(n2)\", time.time() - t)  #"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "taille des nuages ground truth: 300\n",
            "nombre de MLP: 10\n",
            "résolution de la grille: 8 ^2 = 64\n",
            "taille des nuages générés: 10 * 64 = 640\n",
            "résolution segmentation: 5 ^3 = 125\n",
            "time 0 loss train 1.8595e+02 loss test 0.00e+00\n",
            "time 1 loss train 1.1707e+02 loss test 0.00e+00\n",
            "time 2 loss train 8.0666e+01 loss test 0.00e+00\n",
            "time 3 loss train 6.6948e+01 loss test 0.00e+00\n",
            "time 4 loss train 6.4388e+01 loss test 0.00e+00\n",
            "\n",
            "temps: \n",
            "loss 31.65546178817749 [13.90408897  4.89564133  4.5984261   4.06980014  4.18750525]\n",
            "tot 82.04366111755371 [40.42584419 11.48423839 10.94003129  9.55288982  9.64065742]\n",
            "ratio 0.3858367771133585 [0.3439406  0.42629221 0.42033025 0.42602817 0.43435889]\n",
            "O(n) 82.13453102111816\n",
            "répartition point parcourus\n",
            "(array([11373,   321,   249,   128,     4,     1,     0,     0,     0,\n",
            "         600]), array([  4. ,  67.6, 131.2, 194.8, 258.4, 322. , 385.6, 449.2, 512.8,\n",
            "       576.4, 640. ]))\n",
            "66.97230987693278\n",
            "drive/data/output_clouds écrit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-e0c428e3e65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstructeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliste_points\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mwrite_clouds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/data/output_clouds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-71eecb78cc75>\u001b[0m in \u001b[0;36mwrite_clouds\u001b[0;34m(path, clouds)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"écrit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclouds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/data/output_clouds'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Y9U1d5MnbKWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35f3504b-2525-49f3-fac0-4ee6ff4e9328"
      },
      "cell_type": "code",
      "source": [
        "os.listdir(\"drive/Colab/data/latentVectors/02933112/0\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.npy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "LODv67T0zyyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95b490c0-c224-4c6b-b1c2-f9953e53d852"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7t1JlkhS0-ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}